---
title: "Assignment 3 - Task 1"
author: "Meet Muchhala"
format: 
  html:
    code-fold: true
    toc: true
    embed-resources: true
editor: visual
theme: pulse
execute: 
  echo: true
  message: false
  warning: false
---

# Prediction Power with Binary Logistic Regression: Palmetto Species

## Introduction
This dataset provides insights into the survival and growth patterns of two distinct species of palmetto, namely Serenoa repens and Sabal etonia, within the premises of the Archbold Biological Station situated in Florida. Both species belong to the category of fan palms and have been subject to meticulous scientific observation. The researchers involved in this study meticulously recorded various attributes pertaining to the growth of these plants, including plant height, canopy dimensions (length and width), number of green leaves, among others. Data collection spanned multiple years, commencing from 1981 and continuing until 1997, with additional data points gathered in 2001 and 2017. The data contains following variables:

- `species`: The species of the palmetto plant, with two levels representing Serenoa repens and Sabal etonia.
- `height`: Plant height in centimeters.
- `length`: Canopy length in centimeters.
- `width`: Canopy width in centimeters.
- `green_lvs`: Number of green leaves on the plant.

**Data source:** Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. <https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5>

## Objectives

The primary objective of this analysis is to ascertain the efficacy of utilizing measurements such as plant height, canopy dimensions, and green leaf count as determinants for classifying unknown specimens into either S. repens or S. etonia. To achieve this, binary logistic regression was employed to construct a classification model capable of distinguishing between the two species based on these measured attributes.

## Approach

### Data Initialization
To begin the analysis, we initialize the necessary libraries, including tidyverse, tidymodels, and rsample. These libraries provide tools for data manipulation, visualization, and modeling. Next, we read in the dataset containing survival, growth, and biomass estimates of two dominant palmetto species in south-central Florida from 1981 to 2017. The dataset includes variables such as species, plant height, canopy length, canopy width, and number of green leaves.

### Exploratory Analysis
Once the dataset is loaded, we conduct exploratory data analysis to gain insights into the distribution and characteristics of the variables. We create visualizations to compare the differences in plant height, canopy length, canopy width, and number of green leaves between the two palmetto species. These visualizations help us understand which variables may be informative for classifying the species.

### Binary Logistic Regression
Following the exploratory analysis, we perform binary logistic regression to model the probability of a palmetto plant belonging to either species Serenoa repens or Sabal etonia. We create two logistic regression models: one using all predictor variables (plant height, canopy length, canopy width, and number of green leaves), and another model excluding canopy length.

### Cross Validation Performance Model
To evaluate the performance of the models, we use repeated cross-validation with ten-fold cross-validation repeated at least ten times. This allows us to assess the models' generalization performance and compare their accuracy, area under the ROC curve, and other metrics. Based on the cross-validation results, we select the better performing model for further analysis.

### Training the best performing model
Once the better performing model is identified, we train it on the entire dataset to obtain the final model parameters. We then use the trained model to classify the palmetto plants in the dataset and evaluate its classification performance. We assess the model's ability to correctly classify plants into their respective species using a 50% cutoff for probability.

### Conclusion and Results
Finally, we summarize the analysis findings and conclusions in a professionally formatted report, including data visualizations, model performance metrics, and a discussion of the classification results. The report aims to provide insights into the feasibility of using plant characteristics for species classification in the context of palmetto plants in south-central Florida.

## Initializing the libraries

```{r}
library(tidyverse) 
library(here)
library(tidymodels)
library(rsample)
library(kableExtra)
library(cowplot)

```

## Reading in the dataset

```{r}

palmetto_raw_df <- read_csv(here('data/palmetto.csv'))

palmetto_df <- palmetto_raw_df %>% 
  select(species, height, length, width, green_lvs) %>% 
  ## selecting the required variables for analysis
  mutate(species = factor(species))

##levels(palmetto_df$species) ## Species 1: Serenoa repens is level 0, Species 2: Sabal etonia is Level 1

```

## Exploratory Analysis

```{r}

ex_plot1 <- palmetto_df %>%
  ggplot(aes(x = height, y = length, color = species)) +
  geom_point() +
  scale_color_manual(values = c("1" = "darkgreen", "2" = "yellow2"),
                     labels = c("Serenoa repens", "Sabal etonia")) +
  labs(x = "Plant Height (cm)", y = "Canopy Length(cm)", color = "Species") +
  theme_minimal()

# Visualization 2: Canopy Width vs Number of Green Leaves
ex_plot2 <- palmetto_df %>%
  ggplot(aes(x = width, y = green_lvs, color = species)) +
  geom_point() +
  scale_color_manual(values = c("1" = "darkgreen", "2" = "yellow2"),
                     labels = c("Serenoa repens", "Sabal etonia")) +
  labs(x = "Canopy Width", y = "Number of Green Leaves", color = "Species") +
  theme_minimal()


# Visualization 3: Height vs Number of green leaves
  
 ex_plot3 <- palmetto_df %>%
  ggplot(aes(x = height, y = green_lvs, color = species)) +
  geom_point() +
  scale_color_manual(values = c("1" = "darkgreen", "2" = "yellow2"),
                     labels = c("Serenoa repens", "Sabal etonia")) +
  labs(x = "Canopy Width", y = "Number of Green Leaves", color = "Species") +
  theme_minimal()
  
  # Visualization 4: Length vs Number of green leaves
  
ex_plot4 <-  palmetto_df %>%
  ggplot(aes(x = length, y = green_lvs, color = species)) +
  geom_point() +
  scale_color_manual(values = c("1" = "darkgreen", "2" = "yellow2"),
                     labels = c("Serenoa repens", "Sabal etonia")) +
  labs(x = "Canopy Width", y = "Number of Green Leaves", color = "Species") +
  theme_minimal()


```

```{r}
#| label: fig-plots
#| fig-cap: "Plots exploring measuring dimensions and intersection with green leaves  *S. repens* and *S. etonia.*"


plot_grid(ex_plot1, ex_plot2, ex_plot3, ex_plot4,
          labels = c("1", "2", "3", "4"),
          ncol = 2,
          vjust = 1)

```

### Takeaways:

We observe in the visualizations that 'green_lvs' is the evident varying parameters when compared to all the other variables. When visualized alongside each of the measuring dimension such as 'length', 'height' and 'width', 'green_lvs' is evidently varying. It is important that take all these variables into account in one of the models. 

## Binary Logistic Regression: Cross Validation

### Formulas 
```{r}
f1 <- species ~ green_lvs + height + width + length
f2 <- species ~ green_lvs + height + width

```

### Checking data proportions

```{r}
#| label: tbl-proportions
#| tbl-cap: "The proportions of Species 1: Serenoa Repens and Species 2: Sabal Etonia are represented in this table"

palmetto_df %>%
  group_by(species) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(prop = n / sum(n)) %>% 
  kbl() %>% 
  kable_classic()
  

## The proportion of both the species in the dataset are almost equal at 50% each

```

Both species have almost equal proportion and thus, there will not be any need for stratfication of data for testing and training the model. We can directly proceed to Cross Validation.

## Binary Logistic Regression Model: Cross Validation

### Data Folding 

```{r}
set.seed(12321)
palmetto_folds <- vfold_cv(palmetto_df, v = 10, repeats = 10)
```

```{r}

## Setting up the model 
blr_mdl <- logistic_reg() %>% 
  set_engine('glm')

# Creating workflows for Model 1 and 2
blr_wf1 <- workflow() %>% 
  add_model(blr_mdl) %>% 
  add_formula(f1) 

blr_wf2 <- workflow() %>% 
  add_model(blr_mdl) %>% 
  add_formula(f2)

# Implementing workflow on the folded dataset
model1_fit_folds <- blr_wf1 %>% 
  fit_resamples(palmetto_folds)
model2_fit_folds <- blr_wf2 %>% 
  fit_resamples(palmetto_folds)
```

### Collecting model metrics

```{r}
#| label: tbl-metrics
#| tbl-cap: "The metrics for predictive performance of both the models are represented in the table. The results for the accuracy and area under the ROC curve are presented for both Model 1 (A) and Model 2 (B)"
#| tbl-subcap: 
#|      - "A. Model 1"
#|      - "B. Model 2"
#| layout-nrow: 2
#| layout-align: center


### Average the predictive performance of the ten models:
collect_metrics(model1_fit_folds) %>% 
  select(-.config) %>% 
  rename(metric = .metric,
         estimator = .estimator,
         standard_error = std_err) %>% 
  kbl() %>% kable_classic()

collect_metrics(model2_fit_folds) %>% 
  select(-.config) %>% 
  rename(metric = .metric,
         estimator = .estimator,
         standard_error = std_err) %>% 
  kbl() %>% kable_classic()
```
### Performance Outcomes

Although Model 1 emerges as the preferred choice based on both the ROC curve and accuracy, Model 2 has great performance metrics too. After analyzing the results from cross-validation, we have determined that Model 1 outperforms Model 2. Specifically, Model 1 exhibits a higher area under the receiver operating characteristic (ROC) curve compared to Model 2 (0.9725 versus 0.9635). As the area under the curve serves as a reliable metric for classification performance, this indicates that Model 1 is the superior classifier. Furthermore, Model 1 demonstrates a slightly higher accuracy rate than Model 2 (91.7% vs 89.9%). 


### Training Performing Model 

On selecting of Model 1 as the better performing model, we will be training the model on the entire dataset.

```{r}
#| label: tbl-model
#| tbl-cap: "The results of the Model 1 as the best performing model are represented here in terms of: Coefficients values, predictor variables, standard errors and p-values."

# Training Model 1 on entire dataset 

model1_fit <- blr_mdl %>% 
  fit(formula = f1, data = palmetto_df)

tidy(model1_fit) %>% 
  kbl() %>% 
  kable_classic()
```

### Training on entire dataset

Training the best performing model 1 on the entire dataset to analyze the prediction and classify the predicted vs. actual species. 

```{r}

## Predictions based on Model 1 fit
species_predict <- palmetto_df %>%
  mutate(predict(model1_fit, new_data = ., type = 'prob'))


finalmodel_predictions <- species_predict %>% 
  mutate(predicted_species = ifelse(.pred_1 >= .50, levels(palmetto_df$species)[1], levels(palmetto_df$species)[2])) 
  

table1 <- table(finalmodel_predictions %>% 
        select(species, predicted_species))  

```

```{r}
#| label: tbl-correctlyclassified
#| tbl-cap: "Model 1 predictions correctly classified (%). For each species, the number of correct predictions and incorrect predictions are shown. Additionally, the table also show the percentage of correctly classified predictions."

library(kableExtra)

conf_matrix <- table(finalmodel_predictions$species, finalmodel_predictions$predicted_species)

# Calculate percentage of correctly classified observations
correct_percentage <- diag(conf_matrix) / rowSums(conf_matrix) * 100

# Add percentage column to the confusion matrix table
conf_matrix_with_percentage <- cbind(conf_matrix, Correct_Percentage = correct_percentage)

# Convert confusion matrix to data frame
conf_df <- as.data.frame(conf_matrix_with_percentage)

# Add row names and column names
rownames(conf_df) <- rownames(conf_matrix_with_percentage)
colnames(conf_df) <- c("Serenoa repens", "Sabal etonia", "% Correctly Classified")

# Print confusion matrix table with percentage using kableExtra
kable(conf_df, "html") %>%
  kable_classic_2()

```

## Conclusion

In the Model 1, the classification of species 1 (S. repens) observations was accurate approximately 91% of the time, while species 2 (S. etonia) observations were classified correctly around 93% of the time. Despite not achieving perfection, the model exhibits a remarkably high level of accuracy. Enhancements to the model's performance could be explored by identifying and incorporating additional robust predictor variables.

